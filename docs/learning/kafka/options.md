# Kafka Consumer 핵심 설정 가이드

Kafka 컨슈머 운영에서 가장 혼동하기 쉽지만, 안정적인 시스템을 위해 반드시 구분해서 이해해야 하는 두 가지 설정입니다.

> **핵심 요약**
> - `session.timeout.ms` → **"연결 상태(Health)"** 체크
> - `max.poll.interval.ms` → **"처리 상태(Processing)"** 체크

---

## 1. session.timeout.ms (연결 생존 신호)

| 항목 | 설명 |
|------|------|
| **담당 스레드** | 백그라운드 스레드 (Heartbeat Thread) |
| **감시 대상** | 컨슈머-브로커 네트워크 연결, 프로세스 생존 여부 |

### 동작 원리

1. 컨슈머는 별도의 백그라운드 스레드를 통해 브로커에게 주기적으로 **Heartbeat**를 전송
2. `session.timeout.ms` 시간 동안 브로커가 하트비트를 받지 못하면:
   - 브로커는 해당 컨슈머가 **죽었다(Crash/Network Error)** 고 판단
   - 즉시 해당 컨슈머를 그룹에서 제외
   - **리밸런싱(Rebalancing)** 시작

### 설정 팁

```properties
# heartbeat.interval.ms의 약 3배로 설정 (권장: 10초~45초)
session.timeout.ms=30000
heartbeat.interval.ms=10000
```

| 설정 | 문제점 |
|------|--------|
| 너무 짧게 | 네트워크 순간 끊김에도 리밸런싱 발생 → 시스템 불안정 |
| 너무 길게 | 실제 장애 감지 지연 → 서비스 복구 늦어짐 |

---

## 2. max.poll.interval.ms (로직 처리 시간)

| 항목 | 설명 |
|------|------|
| **담당 스레드** | 메인 스레드 (Processing Thread) |
| **감시 대상** | 비즈니스 로직 수행 여부 (무한 루프, 데드락 감지) |

### 동작 원리

1. 컨슈머는 `poll()`을 호출하여 메시지를 가져오고, 로직 처리 후 다시 `poll()` 호출
2. 이전 `poll()`과 다음 `poll()` 호출 사이의 간격이 설정값을 초과하면:
   - 브로커는 컨슈머가 **Livelock/Hang 상태**라고 판단
   - 컨슈머를 그룹에서 제외
   - **리밸런싱** 시작

### 설정 팁

```properties
# 기본값: 5분 (헤비한 작업 시 증가 필요)
max.poll.interval.ms=300000

# 권장: 값을 늘리기보다 한 번에 가져오는 개수를 줄이기
max.poll.records=100
```

> **가장 많이 발생하는 리밸런싱 원인**
> DB 저장, 외부 API 호출 등 처리 시간이 오래 걸리면 이 값을 초과하게 됩니다.
> `max.poll.records`를 줄여서 한 번의 poll() 사이클이 빨리 돌도록 하는 것이 정석입니다.

---

## 비교 요약

| 구분 | session.timeout.ms | max.poll.interval.ms |
|------|-------------------|---------------------|
| **감시 대상** | 물리적 상태 (네트워크, 프로세스 생존) | 논리적 상태 (비즈니스 로직 수행 여부) |
| **담당 스레드** | 백그라운드 하트비트 스레드 | 메인 처리 스레드 |
| **장애 판단** | 컨슈머 셧다운, 네트워크 끊김 | 무한 루프, 데드락, 처리 지연 |
| **주요 원인** | JVM Crash, 네트워크 단절 | DB 타임아웃, 대량 데이터 처리, 외부 API 지연 |
| **대응 방법** | `heartbeat.interval.ms` 조정 | `max.poll.records` 감소 또는 설정값 증가 |

---

## 면접 Q&A

### Q. 카프카 파티션 개수는 몇 개로 설정했고, 왜 그렇게 했나요?

> **핵심 포인트**: 파티션 수 = 컨슈머의 병렬 처리량(Concurrency)과 직결

**답변 예시**:
> "기본적으로 파티션 개수는 **3개**로 설정하여 운영했습니다.
> 당시 운영 중인 백엔드 서버(컨슈머 그룹)가 3개의 인스턴스로 오토스케일링 되고 있었기 때문입니다.
> 파티션과 컨슈머 스레드를 1:1로 매핑하여 처리량을 극대화하면서도, 리밸런싱 비용을 최소화했습니다.
>
> 파티션은 **늘리는 건 쉽지만 줄이는 건 불가능**하기 때문에, 초기에는 보수적으로 3개로 시작하여 **Lag 모니터링**을 통해 증설하는 전략을 취했습니다."

---

### Q. 레플리카(Replication Factor)와 ISR 설정은 어떻게 했나요?

> **핵심 포인트**: 고가용성(HA)과 데이터 유실 방지에 대한 이해도

**답변 예시**:
> "**Replication Factor=3**, **min.insync.replicas=2**로 설정했습니다.
>
> - 브로커 하나가 장애가 나더라도 서비스가 중단되지 않아야 하므로 복제본 3개 필요
> - 데이터 정합성을 위해 최소 2개의 브로커에 데이터가 저장되었음이 확인되어야 ack 전송
>
> 이를 통해 브로커 1대가 다운되더라도 운영에 지장이 없고, 데이터 유실 가능성을 거의 0에 가깝게 만들었습니다."

---

### Q. acks 설정은 어떻게 했나요?

> **핵심 포인트**: 프로젝트 성격에 따라 다르게 설정했음을 보여주기

**답변 예시**:
> "프로젝트 성격에 따라 다르게 적용했습니다.
>
> | 프로젝트 | acks 설정 | 이유 |
> |----------|-----------|------|
> | IoT 센서 데이터 | `acks=1` | 처리 속도 중요, 일부 유실 허용 |
> | 핀테크 결제 로그 | `acks=all` | 데이터 유실 절대 불가, 신뢰성 최우선 |"
