1. MQTT vs WebSocket: 차량 환경에서의 결정적 차이 3가지
면접 답변 시 이 세 가지 키워드를 중심으로 설명하세요: "재연결 비용", "신뢰성(QoS)", "N:M 통신 패턴"

① 연결 복구와 세션 유지 (Session Takeover)
WebSocket: 웹 브라우저를 위해 만들어졌습니다. 터널에 들어가서 연결이 끊기면, 다시 3-Way Handshake를 하고 HTTP 업그레이드 요청을 보내는 무거운 재연결 과정이 필요합니다. 연결이 끊긴 동안의 데이터는 애플리케이션 레벨에서 직접 관리하지 않으면 사라집니다.

MQTT: Clean Session = false 옵션을 사용하면, 기기가 잠깐 연결이 끊겼다 다시 붙어도 브로커가 "아, 너 아까 걔구나"라고 바로 인식합니다. 재연결 오버헤드가 극도로 적고, 끊겨 있던 동안의 메시지도 브로커가 보관했다가 다시 줍니다. (차량 환경 필수 기능)

② 신뢰성 보장 (QoS vs 직접 구현)
WebSocket: 기본적으로 **"보내고 끝(Fire and Forget)"**이거나 TCP에 의존합니다. "메시지가 진짜 도착했는지 확인하고, 안 갔으면 재전송해라"라는 로직(ACK, Retry)을 개발자가 직접 코드로 다 짜야 합니다.

MQTT: 프로토콜 레벨에서 QoS 1(적어도 한 번 전송), QoS 2(정확히 한 번 전송)를 지원합니다. 설정값 하나로 메시지 도달을 보장할 수 있는데, 굳이 웹소켓 위에서 바퀴를 다시 발명(Reinventing the wheel)할 필요가 없다고 판단했습니다.

③ Pub/Sub 패턴의 유연성
WebSocket: 기본적으로 1:1(Point-to-Point) 통신입니다. 채팅방처럼 N:M 통신을 하려면 STOMP 같은 서브 프로토콜을 또 얹어야 해서 헤더가 무거워집니다.

MQTT: 태생이 Pub/Sub입니다. 서버가 몇 대가 되든, 기기가 몇 대가 되든 토픽(Topic) 기반으로 완벽하게 결합도(Coupling)가 분리되어 있어 서버 확장에 훨씬 유리합니다.



면접관에게 **"무지성으로 MQTT를 고른 게 아니라, 다 비교해 보고 최적의 선택을 했다"**는 것을 보여주는 답변입니다.

[답변 예시]

"네, 프로젝트 설계 초기 단계에서 HTTP Polling, SSE(Server-Sent Events), WebSocket을 비교 검토했습니다.

HTTP Polling: 차량 수천 대가 초 단위로 서버에 요청을 보내는 것은 서버 리소스를 너무 낭비하고 실시간성도 떨어진다고 판단해 제외했습니다.

SSE: 서버에서 차량으로 명령을 내리는 건 좋지만, 차량에서 서버로 데이터를 보내는 것이 불가능한 단방향 통신이라 제외했습니다.

WebSocket: 가장 강력한 경쟁 후보였습니다. 하지만 다음 세 가지 이유로 MQTT가 더 적합하다고 결론 내렸습니다.

첫째, 헤더 크기입니다. 웹소켓은 초기 핸드쉐이크가 무겁지만, MQTT는 최소 2바이트 헤더로 동작해 LTE 데이터 비용 절감에 유리합니다.

둘째, 구현 복잡도입니다. 터널 진입 등으로 네트워크가 끊겼을 때의 '메시지 보관 및 재전송(QoS)' 로직을 웹소켓으로 직접 구현하는 것보다, MQTT의 검증된 QoS 기능을 쓰는 것이 안정적이라 판단했습니다.

셋째, 배터리 효율입니다. 모바일 환경에 최적화된 MQTT가 Keep-Alive 패킷 관리가 더 효율적이어서, 차량 배터리(주차 녹화 모드 등) 소모를 줄이는 데 유리합니다.

결론적으로, 웹이 아닌 임베디드/모바일 환경의 불확실성을 커버하기에는 MQTT가 최적의 선택이었습니다."

마지막으로 기술적 깊이를 보여주는 멘트를 덧붙이세요.

"물론, 웹 기반의 관제 대시보드(어드민 페이지)에서는 브라우저 호환성을 위해 MQTT over WebSocket을 사용하여, 브라우저가 별도의 변환 서버 없이 MQTT 브로커와 직접 통신할 수 있도록 아키텍처를 유연하게 가져갔습니다."

설명: MQTT는 TCP 위에서 돌지만, 웹소켓 위에서도 돌 수 있습니다. (NanoMQ도 지원함). 이걸 언급하면 "아, 이 친구는 웹(프론트)과 기기(임베디드)의 차이를 명확히 아는구나"라고 생각하게 됩니다.


단순히 "패킷이 작아서요"라고 답하는 것을 넘어, **배터리 소모 원리(Radio Wake-up)**와 프로토콜 설계의 차이로 설명해야 시니어다운 답변이 됩니다.

면접관을 설득할 수 있는 3가지 핵심 이유를 정리해 드립니다.

1. "2바이트"의 마법 (Protocol Overhead)
가장 직관적인 차이는 데이터의 크기입니다.

MQTT (PINGREQ):

MQTT의 Keep-Alive 패킷(PINGREQ)은 정확히 2바이트입니다. (11000000 00000000)

더 줄일 수 없는, 물리적인 최소 단위입니다. 추가적인 헤더나 페이로드가 아예 없습니다.

WebSocket (Ping Frame vs App-level Ping):

Ping Frame (표준): 웹소켓도 스펙상 Ping/Pong 프레임(제어 프레임)이 존재하며 약 2~6바이트로 매우 작습니다. 하지만...

App-level Ping (현실): 많은 웹소켓 라이브러리나 브라우저 JS API가 Ping 프레임을 개발자에게 직접 노출하지 않습니다. 그래서 개발자들은 보통 JSON으로 하트비트를 직접 구현합니다.

예: {"type": "ping", "timestamp": 17000000} -> 약 40~50바이트 이상

결과적으로 MQTT가 수십 배 더 가벼운 데이터를 보냅니다.

2. 모바일 네트워크의 "RRC 상태"와 배터리 (핵심)
이 부분이 차량용/모바일 개발자로서 가장 중요한 지식입니다.

스마트폰이나 LTE 모뎀은 배터리를 아끼기 위해 RRC(Radio Resource Control) 상태를 가집니다.

IDLE: 데이터 전송 없음. (저전력)

CONNECTED: 데이터 전송 중. (고전력)

데이터를 1바이트라도 보내려면, 모뎀은 IDLE → CONNECTED로 깨어나야 합니다. 이때 전력을 많이 씁니다. 그리고 데이터를 다 보낸 후에도, "혹시 더 보낼 거 있나?" 하고 일정 시간(Tail Time) 동안 고전력 상태를 유지하다가 다시 잡니다.

MQTT의 효율성:

패킷이 극도로 작기 때문에 전송 시간이 짧습니다.

프로토콜 레벨에서 **Keep-Alive 주기(Interval)**를 아주 정교하게(초 단위) 설정할 수 있습니다.

통신사 NAT 타임아웃(보통 1분~5분)이 끊기지 않을 정도로만 최소한의 횟수로 깨어나도록 튜닝하기가 매우 쉽습니다.

3. 처리 로직의 단순성 (CPU Cycle = Battery)
MQTT:

브로커와 클라이언트는 2바이트(0xC0 0x00)만 확인하면 됩니다. 비트 연산 한 번이면 끝납니다. 임베디드 기기의 CPU를 거의 쓰지 않습니다.

WebSocket:

프레임을 받으면 마스킹(Masking)을 해제하고, 페이로드를 파싱해야 합니다.

만약 JSON으로 직접 핑을 구현했다면? 문자열을 파싱해서 JSON 객체로 만들고 내용을 확인해야 합니다. 이 과정에서 CPU 사이클을 더 소모하고, 이는 곧 배터리 소모로 이어집니다.

[면접용 답변 요약]
면접관에게 이렇게 설명하시면 됩니다.

"웹소켓도 Ping 프레임이 있지만, MQTT의 Keep-Alive 관리가 더 효율적인 이유는 패킷 사이즈와 구현의 강제성 때문입니다.

최소한의 데이터: MQTT의 PINGREQ는 헤더 포함 단 2바이트입니다. 반면 웹소켓은 라이브러리 제약상 애플리케이션 레벨에서 JSON 핑을 보내는 경우가 많아 오버헤드가 큽니다.

배터리 효율: 모바일 네트워크(LTE) 환경에서는 데이터를 전송할 때마다 무선 칩셋(Radio)이 고전력 상태로 깨어납니다(Wake-up). MQTT는 가장 작은 패킷으로, NAT가 끊기지 않을 최소한의 주기로만 통신하여 Radio 사용 시간과 Tail Time 전력 소모를 최소화할 수 있습니다.

차량의 배터리 방전을 막아야 하는 대시캠 환경에서는, 이러한 미세한 효율성 차이가 중요하다고 판단했습니다."

이 정도 깊이면 면접관도 "네트워크 레벨까지 고민했구나"라고 인정할 것입니다.

--
**"그냥 문자열 그대로 토스(Pass-through)하면 CPU도 안 쓰고 제일 빠른데, 굳이 왜 Gatherer에서 파싱하고 검증해서 CPU를 낭비했나요?"**

논리 1. "확장성을 위한 표준화 (Normalization)" [가장 강력한 답변]
"Gatherer의 역할은 단순 수집이 아니라, 외부 의존성을 격리하는 '부패 방지 계층(Anti-Corruption Layer)'이기 때문입니다."

상황: 지금은 업비트 하나지만, 나중에 **바이낸스(Binance), 빗썸(Bithumb)**이 추가된다면?

문제: 업비트는 trade_price라고 주고, 바이낸스는 p(price)라고 줍니다. 이걸 그대로 Pass-through 해서 내부 시스템(Redis, API 서버, DB)까지 흘려보내면, 내부의 모든 로직이 외부 거래소의 JSON 포맷에 의존하게 됩니다.

해결: Gatherer가 **"어디서 들어오든 우리 시스템 내부 표준 포맷(Canonical Model)으로 변환"**해서 넘겨줘야 합니다.

답변 예시:

"만약 Raw 데이터를 그대로 내부망에 흘리면, API 서버가 업비트, 바이낸스, 빗썸의 JSON 형식을 모두 알고 있어야 합니다. 저는 Gatherer가 '통역사' 역할을 해야 한다고 판단했습니다. 외부 데이터가 들어오는 즉시 우리 서비스의 표준 규격(DTO/Protobuf)으로 변환하여, 하위 시스템들은 거래소가 어디인지 몰라도 되게끔 **결합도(Coupling)**를 끊었습니다."

논리 2. "네트워크 및 메모리 비용 절감 (Filtering)"
"불필요한 데이터가 내부망을 점유하는 것을 막기 위해서입니다."

상황: 업비트가 보내주는 JSON에는 timestamp, stream_type, sequential_id 등 우리가 화면에 안 보여줄 데이터도 잔뜩 들어있습니다. (약 30~40%가 불필요한 필드)

문제: 이걸 그대로 Redis에 저장하고 API 서버로 보내면, 안 쓰는 데이터 때문에 Redis 메모리 비용과 네트워크 대역폭(Bandwidth)이 낭비됩니다.

해결: Gatherer에서 필요한 필드(현재가, 등락률 등)만 딱 뽑아서(Parsing & Filtering) 다시 조립해 보내는 게 전체 인프라 비용 면에서 이득입니다.

답변 예시:

"업비트 응답 패킷을 분석해 보니 실제 서비스에 필요한 필드는 전체의 60% 수준이었습니다. Raw 데이터를 그대로 Redis에 저장하면 불필요한 필드 때문에 메모리와 네트워크 대역폭 낭비가 발생한다고 판단했습니다. 그래서 Gatherer에서 필요한 데이터만 정제(Filtering)하여 페이로드 크기를 줄였습니다."

논리 3. "데이터 무결성 보장 (Fail-Fast)"
"쓰레기 데이터가 하류 시스템(Downstream)을 오염시키는 것을 입구 컷 하기 위해서입니다."

상황: 만약 업비트 오류로 가격에 null이 오거나 -1 같은 이상한 값이 온다면?

문제: 이걸 그대로 흘려보내면, 프론트엔드 차트가 깨지거나 자동매매 로직이 오작동해서 금전적 손실이 날 수 있습니다. 에러는 발생한 곳에서 가장 가까운 곳에서 잡아야 합니다.

해결: Gatherer가 "가격은 0보다 커야 한다", "코인 코드는 필수다" 같은 기본 검증을 수행하고, 이상하면 버려야 합니다.

답변 예시:

"잘못된 데이터(Poison Pill)가 DB나 클라이언트까지 도달하면 장애 파급력이 커집니다. 저는 Gatherer를 **데이터 유입의 관문(Gatekeeper)**으로 정의하고, 스키마 검증을 통과한 신뢰할 수 있는 데이터만 내부 시스템으로 전파하도록 설계했습니다."

[면접 시나리오 요약]
면접관: "Gatherer에서 왜 굳이 파싱 비용을 들였나요?"

작성자님: "네, 단순히 문자열을 넘기는 게 당장의 성능은 빠를 수 있습니다. 하지만 저는 시스템의 유지보수성과 데이터 품질이 더 중요하다고 판단했습니다.

첫째, 확장성입니다. 추후 바이낸스 등 타 거래소가 추가될 때 내부 시스템 변경 없이 Gatherer만 수정하면 되도록 표준 포맷으로 정규화했습니다.

둘째, 비용 최적화입니다. 불필요한 필드를 제거하여 Redis와 네트워크 대역폭을 절약했습니다.

셋째, 안정성입니다. 잘못된 데이터가 유입되는 것을 입구에서 차단(Fail-Fast)했습니다.

이 과정에서 발생한 CPU 부하(JSON 파싱 비용)가 병목이 되는 것을 확인했고, 이를 해결하기 위해 **gRPC(Protobuf)**를 도입하여 직렬화 비용과 데이터 크기를 동시에 잡는 방식으로 최적화했습니다."